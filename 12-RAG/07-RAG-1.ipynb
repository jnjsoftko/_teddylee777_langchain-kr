{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b82288e",
   "metadata": {},
   "source": [
    "# LangChain을 활용한 간단한 Q&A 애플리케이션 만들기\n",
    "\n",
    "## 개요\n",
    "\n",
    "LangChain은 질문과 답변(Q&A) 애플리케이션을 구축하는 데 필요한 다양한 구성 요소를 제공하는 툴킷입니다. 이러한 구성 요소들은 개발자가 텍스트 데이터 소스를 활용하여 고급 Q&A 애플리케이션을 쉽게 만들 수 있도록 돕습니다. 본 튜토리얼에서는 LangChain을 사용하여 간단한 Q&A 애플리케이션을 구축하는 과정을 소개하고, LangChain의 주요 구성 요소와 이들이 어떻게 상호 작용하는지 살펴보겠습니다. 또한, 애플리케이션 개발 과정에서 LangSmith가 어떻게 도움이 될 수 있는지도 탐구할 것입니다.\n",
    "\n",
    "## LangChain Q&A 아키텍처\n",
    "\n",
    "LangChain Q&A 애플리케이션의 아키텍처는 크게 두 가지 주요 단계로 구성됩니다: **질문 처리**와 **답변 생성**. 이 과정에서 여러 LangChain 구성 요소가 사용됩니다.\n",
    "\n",
    "### 1. 질문 처리\n",
    "\n",
    "질문 처리 단계에서는 사용자의 질문을 받아 이를 처리하고, 관련 데이터를 찾는 작업이 이루어집니다. 이를 위해 다음과 같은 구성 요소들이 필요합니다:\n",
    "\n",
    "- **데이터 소스 연결**: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.\n",
    "- **데이터 인덱싱 및 검색**: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.\n",
    "\n",
    "### 2. 답변 생성\n",
    "\n",
    "관련 데이터를 찾은 후에는 이를 기반으로 사용자의 질문에 답변을 생성해야 합니다. 이 단계에서는 다음 구성 요소가 중요합니다:\n",
    "\n",
    "- **답변 생성 모델**: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.\n",
    "\n",
    "**LangSmith의 역할**\n",
    "\n",
    "LangSmith는 애플리케이션의 성능을 추적하고 분석하는 데 도움을 주는 도구입니다. Q&A 애플리케이션의 복잡성이 증가함에 따라, LangSmith는 다음과 같은 방법으로 유용하게 사용될 수 있습니다:\n",
    "\n",
    "- **성능 모니터링**: LangSmith를 사용하여 애플리케이션의 질문 처리 및 답변 생성 성능을 모니터링할 수 있습니다. 이를 통해 애플리케이션의 성능을 지속적으로 개선할 수 있습니다.\n",
    "- **오류 분석**: LangSmith는 애플리케이션이 잘못된 답변을 생성했을 때, 이의 원인을 분석하는 데 도움을 줍니다. 이를 통해 개발자는 애플리케이션의 오류를 정확하게 파악하고, 이를 수정하는 데 필요한 정보를 얻을 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "LangChain을 사용하여 간단한 Q&A 애플리케이션을 구축하는 과정은 개발자에게 매우 유용한 경험을 제공합니다. LangChain의 다양한 구성 요소는 질문 처리와 답변 생성 과정을 간소화하며, LangSmith는 애플리케이션의 성능을 모니터링하고 개선하는 데 필수적인 도구입니다. 이 튜토리얼을 통해 얻은 지식을 바탕으로, 개발자는 자신만의 고급 Q&A 애플리케이션을 구축할 수 있는 기반을 마련할 수 있을 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6f1b5",
   "metadata": {},
   "source": [
    "## 아키텍처\n",
    "\n",
    "우리는 [Q&A 소개](https://python.langchain.comhttps://python.langchain.com/docs/use_cases/question_answering/)에서 개요한 대로 전형적인 RAG 애플리케이션을 만들 것입니다. 이것은 두 가지 주요 구성 요소를 가지고 있습니다:\n",
    "\n",
    "- **인덱싱**: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다. _이 작업은 보통 오프라인에서 발생합니다._\n",
    "\n",
    "- **검색 및 생성**: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.\n",
    "\n",
    "RAW 데이터에서 답변을 받기까지의 전체 순서는 다음과 같습니다.\n",
    "\n",
    "### 인덱싱\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n",
    "\n",
    "1. **로드**: 먼저 데이터를 로드해야 합니다. 이를 위해 [DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)를 사용할 것입니다.\n",
    "2. **분할**: [Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)는 큰 `Documents`를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하고 모델에 전달하는 데 유용하며, 큰 청크는 검색하기 어렵고 모델의 유한한 컨텍스트 창에 맞지 않습니다.\n",
    "3. **저장**: 나중에 검색할 수 있도록 분할을 저장하고 인덱싱할 장소가 필요합니다. 이는 종종 [VectorStore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)와 [Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) 모델을 사용하여 수행됩니다.\n",
    "\n",
    "### 검색 및 생성\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n",
    "\n",
    "1. **검색**: 사용자 입력이 주어지면 [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/)를 사용하여 저장소에서 관련 분할을 검색합니다.\n",
    "2. **생성**: [ChatModel](https://python.langchain.com/docs/modules/model_io/chat/) / [LLM](https://python.langchain.com/docs/modules/model_io/llms/)은 질문과 검색된 데이터를 포함한 프롬프트를 사용하여 답변을 생성합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c423a8",
   "metadata": {},
   "source": [
    "## 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224fd32",
   "metadata": {},
   "source": [
    "API KEY 를 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418ab505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024d0c5",
   "metadata": {},
   "source": [
    "LangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은 [LangSmith](https://smith.langchain.com)를 사용하는 것입니다.\n",
    "\n",
    "LangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edbbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 디버깅을 위한 프로젝트명을 기입합니다.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"4일차 실습\"\n",
    "\n",
    "# tracing 을 위해서는 아래 코드의 주석을 해제하고 실행합니다.\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de11a49",
   "metadata": {},
   "source": [
    "## 첫 번째 RAG 기반 QA봇\n",
    "\n",
    "첫 번째 튜토리얼에는 네이버 뉴스기사의 내용에 대해 질문할 수 있는 **뉴스기사 QA 앱** 을 구축할 것입니다.\n",
    "\n",
    "이 가이드에서는 OpenAI 챗 모델과 임베딩, 그리고 Chroma 벡터 스토어를 사용할 것입니다.\n",
    "\n",
    "하지만 여기서 보여지는 모든 것은 어떤 [ChatModel](https://python.langchain.com/docs/modules/model_io/chat/)이나 [LLM](https://python.langchain.com/docs/modules/model_io/llms/), [임베딩](https://python.langchain.com/docs/modules/data_connection/text_embedding/), 그리고 [VectorStore](https://python.langchain.com/docs/modules/data_connection/vectorstores/) 또는 [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/)와도 작동합니다.\n",
    "\n",
    "먼저 다음의 과정을 통해 간단한 인덱싱 파이프라인과 RAG 체인을 약 20줄의 코드로 구현할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ed3df",
   "metadata": {},
   "source": [
    "라이브러리\n",
    "\n",
    "- `bs4`는 웹 페이지를 파싱하기 위한 라이브러리입니다.\n",
    "- `langchain`은 AI와 관련된 다양한 기능을 제공하는 라이브러리로, 여기서는 특히 텍스트 분할(`RecursiveCharacterTextSplitter`), 문서 로딩(`WebBaseLoader`), 벡터 저장(`Chroma`, `FAISS`), 출력 파싱(`StrOutputParser`), 실행 가능한 패스스루(`RunnablePassthrough`) 등을 다룹니다.\n",
    "- `langchain_openai` 모듈을 통해 OpenAI의 챗봇(`ChatOpenAI`)과 임베딩(`OpenAIEmbeddings`) 기능을 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d1b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595217a5",
   "metadata": {},
   "source": [
    "웹 페이지의 내용을 로드하고, 텍스트를 청크로 나누어 인덱싱하는 과정을 거친 후, 관련된 텍스트 스니펫을 검색하여 새로운 내용을 생성하는 과정을 구현합니다.\n",
    "\n",
    "`WebBaseLoader`는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해 `bs4.SoupStrainer`를 사용합니다.\n",
    "\n",
    "[참고]\n",
    "\n",
    "- `bs4.SoupStrainer` 는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.\n",
    "\n",
    "(예시)\n",
    "\n",
    "```python\n",
    "bs4.SoupStrainer(\n",
    "    \"div\",\n",
    "    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]}, # 클래스 명을 입력\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f69f249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"\\n출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\\n\\n\\n[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨습니다.2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다.해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다.[오현석/부영그룹 직원 : 아이 키우는 데 금전적으로 많이 힘든 세상이잖아요. 교육이나 생활하는 데 큰 도움이 될 거라 생각합니다.]만약 셋째까지 낳는 경우엔 국민주택을 제공하겠다는 뜻도 밝혔습니다.[이중근/부영그룹 회장 : 3년 이내에 세 아이를 갖는 분이 나올 것이고 따라서 주택을 제공할 수 있는 계기가 될 것으로 생각하고.][조용현/부영그룹 직원 : 와이프가 셋째도 갖고 싶어 했는데 경제적 부담 때문에 부정적이었거든요. (이제) 긍정적으로 생각할 수 있을 것 같습니다.]오늘 행사에서는, 회사가 제공하는 출산장려금은 받는 직원들의 세금 부담을 고려해 정부가 면세해달라는 제안도 나왔습니다.이같은 출산장려책은 점점 확산하는 분위기입니다.법정기간보다 육아휴직을 길게 주거나, 남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]\\n\\t\\t\\n\", metadata={'source': 'https://n.news.naver.com/article/437/0000378416'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://n.news.naver.com/article/437/0000378416\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\",\n",
    "                             \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d5bce",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter`는 문서를 지정된 크기의 청크로 나눕니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9bf670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a38783",
   "metadata": {},
   "source": [
    "`FAISS` 혹은 `Chroma`와 같은 vectorstore는 이러한 청크를 바탕으로 문서의 벡터 표현을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a8ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f951a",
   "metadata": {},
   "source": [
    "`vectorstore.as_retriever()`를 통해 생성된 검색기는 `hub.pull`로 가져온 프롬프트와 `ChatOpenAI` 모델을 사용하여 새로운 내용을 생성합니다.\n",
    "\n",
    "마지막으로, `StrOutputParser`는 생성된 결과를 문자열로 파싱합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee334712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508710e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d16128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-turbo-preview\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d6d5c",
   "metadata": {},
   "source": [
    "이 함수는 `rag_chain` 객체의 `invoke` 메서드를 사용하여 사용자의 질문을 처리합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948a029",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/0413a024-03e4-46d0-b1f3-0998040ffe90/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3744c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부영그룹은 출산 장려 정책으로 2021년 이후 태어난 직원 자녀에게 1억원씩 지원하며, 연년생이나 쌍둥이 자녀가 있을 경우 총 2억원을 지급합니다. 또한, 셋째 아이를 낳는 경우 국민주택을 제공하겠다는 계획도 밝혔습니다. 이러한 파격적인 저출생 정책은 직원들에게 금전적인 지원 뿐만 아니라 주거 지원까지 포함하여 출산과 양육을 장려합니다."
     ]
    }
   ],
   "source": [
    "_ = rag_chain.invoke(\n",
    "    \"부영그룹의 출산 장려 정책에 대해 설명해주세요.\"\n",
    ")  # 문서에 대한 질의를 입력하고, 답변을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eec33e",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/7e77fb8e-9c93-4944-9245-3079f32fbb38/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b406c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부영그룹은 출산 직원에게 1억원을 지원합니다. 2021년 이후 태어난 직원 자녀에게 이 지원금을 제공하며, 연년생이나 쌍둥이 자녀가 있을 경우 총 2억원을 받게 됩니다. 셋째 아이를 낳는 경우에는 국민주택을 제공하겠다는 계획도 밝혔습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'부영그룹은 출산 직원에게 1억원을 지원합니다. 2021년 이후 태어난 직원 자녀에게 이 지원금을 제공하며, 연년생이나 쌍둥이 자녀가 있을 경우 총 2억원을 받게 됩니다. 셋째 아이를 낳는 경우에는 국민주택을 제공하겠다는 계획도 밝혔습니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"부영그룹은 출산 직원에게 얼마의 지원을 제공하나요?\"\n",
    ")  # 문서에 대한 질의를 입력하고, 답변을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044ceba",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/e52cc340-742f-4fcf-a320-8fc2cc882161/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72780dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 정부 저출생 대책으로 0세 아이에게 매달 100만원의 부모 급여 지급\n",
      "- 첫만남이용권 및 아동수당 포함하여 아이 돌까지 1년 동안 총 1520만원 지원\n",
      "- 지자체별 추가 지원 예: 인천시는 새로 태어난 아기에게 18살 될 때까지 1억원 지급, 광주시는 17살 될 때까지 7400만원 지급- 정부 저출생 대책으로 0세 아이에게 매달 100만원의 부모 급여 지급\n",
      "- 첫만남이용권 및 아동수당 포함하여 아이 돌까지 1년 동안 총 1520만원 지원\n",
      "- 지자체별 추가 지원 예: 인천시는 새로 태어난 아기에게 18살 될 때까지 1억원 지급, 광주시는 17살 될 때까지 7400만원 지급\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    rag_chain.invoke(\"정부의 저출생 대책을 bullet points 형식으로 작성해 주세요.\")\n",
    ")  # 문서에 대한 질의를 입력하고, 답변을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c627d96",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/1810ccbe-cc6a-42c4-adee-741e44a9ffe3/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6845bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제공된 문맥에서 부영그룹의 임직원 수에 대한 구체적인 정보는 언급되지 않았습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'제공된 문맥에서 부영그룹의 임직원 수에 대한 구체적인 정보는 언급되지 않았습니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"부영그룹의 임직원 숫자는 몇명인가요?\"\n",
    ")  # 문서에 대한 질의를 입력하고, 답변을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90743f1b",
   "metadata": {},
   "source": [
    "이 함수는 `vectorstore` 객체의 `delete_collection` 메서드를 호출하여 컬렉션을 삭제합니다. 이는 데이터 정리 과정에서 사용되며, 특정 데이터셋이나 정보를 저장하는 컬렉션을 제거함으로써 시스템의 저장 공간을 확보하고, 불필요한 데이터로 인한 혼란을 방지합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36aa3aba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No ids provided to delete.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 컬렉션을 삭제합니다.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# vectorstore.delete_collection()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:816\u001b[0m, in \u001b[0;36mFAISS.delete\u001b[0;34m(self, ids, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Delete by ID. These are the IDs in the vectorstore.\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \n\u001b[1;32m    808\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    False otherwise, None if not implemented.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo ids provided to delete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    817\u001b[0m missing_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ids)\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_docstore_id\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_ids:\n",
      "\u001b[0;31mValueError\u001b[0m: No ids provided to delete."
     ]
    }
   ],
   "source": [
    "# 컬렉션을 삭제합니다.\n",
    "vectorstore.delete_collection()\n",
    "# vectorstore.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
