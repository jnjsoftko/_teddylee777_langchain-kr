{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bf28f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18febf",
   "metadata": {},
   "source": [
    "설치된 openai 의 버전을 체크합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550920c-09d8-48b3-be2f-b36362c37989",
   "metadata": {},
   "source": [
    "## Gemini\n",
    "\n",
    "Google 사의 채팅 전용 Large Language Model(llm) 입니다.\n",
    "\n",
    "객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.\n",
    "\n",
    "`temperature`\n",
    "\n",
    "- 사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.\n",
    "\n",
    "`max_tokens`\n",
    "\n",
    "- 채팅 완성에서 생성할 토큰의 최대 개수입니다.\n",
    "\n",
    "`model_name`: 적용 가능한 모델 리스트\n",
    "\n",
    "- 링크: https://gemini.google.com/app\n",
    "\n",
    "- install\n",
    "```sh\n",
    "# pip install -q -U google-generativeai\n",
    "pip install -q --upgrade google-generativeai langchain-google-genai\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc161c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[답변]: content='서울' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-ecfd2f16-f88e-4b16-a18e-1c699b5a5209-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# 질의내용\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# 질의\n",
    "print(f\"[답변]: {llm.invoke(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d74d9",
   "metadata": {},
   "source": [
    "## 프롬프트 템플릿의 활용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8aedc",
   "metadata": {},
   "source": [
    "`PromptTemplate`\n",
    "\n",
    "- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\n",
    "- 사용법\n",
    "  - `template`: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 `{}`는 변수를 나타냅니다.\n",
    "  - `input_variables`: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\n",
    "\n",
    "`input_variables`\n",
    "\n",
    "- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\n",
    "- 사용법: 리스트 형식으로 변수 이름을 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b3f01e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], template='{country}의 수도는 뭐야?')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 질문 템플릿 형식 정의\n",
    "template = \"{country}의 수도는 뭐야?\"\n",
    "\n",
    "# 템플릿 완성\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d700a6e",
   "metadata": {},
   "source": [
    "### LLMChain 객체\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3cc045",
   "metadata": {},
   "source": [
    "`LLMChain`\n",
    "\n",
    "- LLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다\n",
    "- 사용법\n",
    "  - `prompt`: 앞서 정의한 PromptTemplate 객체를 사용합니다.\n",
    "  - `llm`: 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "572eb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 연결된 체인(Chain)객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6369fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': '대한민국', 'text': '서울'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"country\": \"대한민국\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd692f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': '캐나다', 'text': '오타와'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"country\": \"캐나다\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9a157",
   "metadata": {},
   "source": [
    "### apply()\n",
    "\n",
    "`apply()` 함수로 여러개의 입력에 대한 처리를 한 번에 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b656d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [{\"country\": \"호주\"}, {\"country\": \"중국\"}, {\"country\": \"네덜란드\"}]\n",
    "\n",
    "response = llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaec81d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'캔버라'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622efee7",
   "metadata": {},
   "source": [
    "`text` 키 값으로 결과 뭉치가 반환되었음을 확인할 수 있습니다.\n",
    "\n",
    "이를 반복문으로 출력한다면 다음과 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01abd39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캔버라\n",
      "베이징\n",
      "암스테르담\n"
     ]
    }
   ],
   "source": [
    "# input_list 에 대한 결과 반환\n",
    "result = llm_chain.apply(input_list)\n",
    "\n",
    "# 반복문으로 결과 출력\n",
    "for res in result:\n",
    "    print(res[\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38852a",
   "metadata": {},
   "source": [
    "### `generate()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd758206",
   "metadata": {},
   "source": [
    "`generate()` 는 문자열 대신에 LLMResult를 반환하는 점을 제외하고는 apply와 유사합니다.\n",
    "\n",
    "LLMResult는 토큰 사용량과 종료 이유와 같은 유용한 생성 정보를 자주 포함하고 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2457653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='캔버라', generation_info={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, message=AIMessage(content='캔버라', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-10459258-c26f-4afd-b388-e1285c228cd2-0'))], [ChatGeneration(text='베이징', generation_info={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, message=AIMessage(content='베이징', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-f1d2b891-6460-420c-b368-ee96c77e4226-0'))], [ChatGeneration(text='암스테르담', generation_info={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, message=AIMessage(content='암스테르담', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-3f0d5fc3-0e5e-4c5d-b5a9-dd82c8675040-0'))]] llm_output={} run=[RunInfo(run_id=UUID('10459258-c26f-4afd-b388-e1285c228cd2')), RunInfo(run_id=UUID('f1d2b891-6460-420c-b368-ee96c77e4226')), RunInfo(run_id=UUID('3f0d5fc3-0e5e-4c5d-b5a9-dd82c8675040'))]\n"
     ]
    }
   ],
   "source": [
    "# 입력값\n",
    "input_list = [{\"country\": \"호주\"}, {\"country\": \"중국\"}, {\"country\": \"네덜란드\"}]\n",
    "\n",
    "# input_list 에 대한 결과 반환\n",
    "generated_result = llm_chain.generate(input_list)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59487318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 사용량 출력\n",
    "generated_result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a99eed",
   "metadata": {},
   "source": [
    "### 2개 이상의 변수를 템플릿 안에 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b1093",
   "metadata": {},
   "source": [
    "2개 이상의 변수를 적용하여 템플릿을 생성할 수 있습니다.\n",
    "\n",
    "이번에는 2개 이상의 변수(`input_variables`) 를 활용하여 템플릿 구성을 해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a9e9a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['area1', 'area2'], template='{area1} 와 {area2} 의 시차는 몇시간이야?')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문 템플릿 형식 정의\n",
    "template = \"{area1} 와 {area2} 의 시차는 몇시간이야?\"\n",
    "\n",
    "# 템플릿 완성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc451ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연결된 체인(Chain)객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3533451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area1': '서울', 'area2': '파리', 'text': '8시간'}\n"
     ]
    }
   ],
   "source": [
    "# 체인 실행: run()\n",
    "print(llm_chain.invoke({\"area1\": \"서울\", \"area2\": \"파리\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78aaa1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6시간\n",
      "20시간\n",
      "2시간\n"
     ]
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"area1\": \"파리\", \"area2\": \"뉴욕\"},\n",
    "    {\"area1\": \"서울\", \"area2\": \"하와이\"},\n",
    "    {\"area1\": \"켄버라\", \"area2\": \"베이징\"},\n",
    "]\n",
    "\n",
    "# 반복문으로 결과 출력\n",
    "result = llm_chain.apply(input_list)\n",
    "for res in result:\n",
    "    print(res[\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef6054",
   "metadata": {},
   "source": [
    "## stream: 실시간 출력\n",
    "\n",
    "스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.\n",
    "\n",
    "다음과 같이 `streaming=True` 로 설정하고 스트리밍으로 답변을 받기 위한 `StreamingStdOutCallbackHandler()` 을 콜백으로 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14dfb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 객체 생성\n",
    "# llm = ChatOpenAI(\n",
    "#     temperature=0,  # 창의성 (0.0 ~ 2.0)\n",
    "#     max_tokens=2048,  # 최대 토큰수\n",
    "#     model_name=\"gpt-3.5-turbo\",  # 모델명\n",
    "#     streaming=True,\n",
    "#     callbacks=[StreamingStdOutCallbackHandler()],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d721f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질의내용\n",
    "question = \"대한민국에 대해서 300자 내외로 최대한 상세히 알려줘\"\n",
    "\n",
    "# 스트리밍으로 답변 출력\n",
    "response = llm.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
